\documentclass[a4paper]{report}
\usepackage[top=25mm,bottom=25mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Dispense essenziali di Probabilità e Statistica}
\author{Matteo Bitussi \\ Laurea in Informatica, Unitn}
\date{Anno accademico 2018-2019}

\begin{document}
  \maketitle
  \section{Introduzione}
  Questa dispensa è pensata per raccogliere le informazioni essenziali necessarie per lo svolgimento degli esercizi durante l'anno e/o per l'esame finale. Per questo motivo non saranno approfondite, e non potranno sostituire quelle fornite dal professore.

  \tableofcontents

  \chapter{Probabilità}

  \section{Insieme delle parti di $\Omega$: $P(\Omega)$}
  Dato l'insieme $\Omega$ si dice \textbf{Insieme delle Parti} o \textbf{Insieme Potenza} di $\Omega$ l'insieme $P(\Omega)$ di tutti i possibili sottoinsiemi di $\Omega$.

  \section{Tribù (o $\sigma$-algebra))}
  Una classe $\mathcal{A}$ di parti di un insieme $\Omega$ si dice una \textbf{Tribù} se:\\
  \begin{itemize}
  \item $\Omega \in \mathcal{A}$
  \item Se $A \in \mathcal{A}$ allora $A^c \in \mathcal{A}$
  \item Se $A_1, ... ,A_i, ... \in \mathcal{A}$ allora $\bigcup\limits_{i=1}^{\infty} F_{i}$
  \end{itemize}

  \section{Spazio Probabilizzabile}
  Dato uno spazio campionario $\Omega$ e una tribù $ \mathcal{A}$ su $\Omega$, la coppia $(\Omega,\mathcal{A})$ è detto \textbf{Spazio Probabilizzabile}

  \section{Definizione di Probabilità}
  Dato uno spazio probabilizzabile $(\Omega,\mathcal{A})$, una \textbf{Probabilità} $Pr$ è un'applicazione $Pr:\mathcal{A} \longrightarrow \mathbb{R}^+$ tale che:
  \begin{itemize}
    \item (non negatività) se $A \in \mathcal{A}$ allora $Pr(A) \geq 0$
    \item (normalizzazione) $Pr(\Omega) = 1$
    \item ($\sigma$-addività) Se ${\{A_i\}}_{i=1}^{\infty}$ è una successione di eventi di $\mathcal{A}$ a due a due incompatibili (cioè $A_i \cap A_j = \emptyset, i \neq j$), allora
    \[ Pr(\bigcup\limits_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} Pr(A_i) \]
  \end{itemize}

  \section{Spazio proabilizzato}
  La terna $(\Omega, \mathcal{A}, Pr)$ dove $\Omega$ è uno spaio campionario, $\mathcal{A}$ è una Tribù su $\Omega$ e $Pr$ è una funzione di probabilità $Pr: \mathcal{A} \longrightarrow \mathbb{R^+}$, è detta \textbf{Spazio di Probabilità} o anche spazio di Kolmogrov.

  \section{Regole di calcolo delle probabilità}
  \subsection{Regola 1}
  Se $A$ è un evento di probabilità $Pr(A)$ allora la probabilità che $A$ non si verifichi è
  \[ Pr(A^c)=1-Pr(A) \]

  \subsection{Regola 2}
  Se $A$ e $B$ sono due eventi, allora la probabilità che se ne verifichi almeno uno è data da
  \[ Pr(A \cup B) = Pr(A) + Pr(B) - Pr(A \cap B) \]

  \subsection{Regola 3}
  Se $A$ è un evento che implica l'evento $B$, cioè se $A \subseteq B$, allora
  \[ Pr(B) = Pr(A) + Pr(B \cap A^c) \geq Pr(A) \]

  \subsection{Regola 4 (Disuguaglianza di Bonferroni)}
  Se $A_1, A_2, ..., A_n$ non sono eventi, allora
  \[ \sum_{i=1}^{n} Pr(A_i) - \sum_{1_\leq i \leq j \leq n} Pr(A_i \cap A_j) \leq Pr(\bigcup\limits_{i=1}^{n} A_i) \leq \sum_{i=1}^{n} Pr(A_i),     n \geq 1 \]

  \chapter{Calcolo combinatorio}
  \section{Disposizioni con ripetizione}
  Dato un insieme $S = {a_1,a_2,...,a_n}$ di $n$ oggetti distinti, il numero degli allineamenti che si possono formare con $k$ oggetti scelti tra gli $n$ - ritenendo diversi due allineamenti, o perchè contengono oggetti differenti o perche gli stessi oggetti si susseguono in ordine diverso o, infine, perchè uno stesso oggetto si ripete un numero diverso di volte - è dato da
  \[ D_{n,k}^* = n^k \]
  Ogni allineamento si dice disposizione con ripetizione di $n$ oggetti di classe $k$.

  \section{Disposizioni senza ripetizione}
  Dato un insieme $S={a_1,a_2,...,a_n}$ di $n$ oggetti distinti, il numero degli allineamenti che si possono formare con $1 \leq k \leq n$ ogetti scelti tra gli $n$ - ritenendo diversi due allineamenti o perchè contengono oggetti differenti o perchè gli stessi oggetti si susseguono in ordine diverso - è dato da
  \[ D_{n,k} = n(n-1)(n-2)...(n-k + 1) \]
  Ogni allineamento si dice disposizione semplice o senza ripetizione di $n$ oggetti di classe $k$

  \section{Permutazioni}
  Dato un insieme $S={a_1,a_2,...,a_n}$ di $n$ oggetti distinti, il numero degli allineamenti che si possono formare con tutti essi - ritenendo diversi due allineamenti perchè gli oggetti si susseguono in ordine diverso - è dato da $n!$

  \section{Combinazioni}
  Dato un insieme $S={a_1,a_2,...,a_n}$ di $n$ oggetti distinti, il numero degli allineamenti che si possono formare con $1 \leq k \leq n$ oggetti scelti tra gli $n$ - ritenendo diversi due allineamenti solo perchè contengono oggetti differenti - è dato da
  \[ C_{n,k} = \frac{D_{n,k}}{k!} \]
  Ogni allineamento si dice combinazione senza ripetizione di $n$ oggetti di classe $k$

  \section{Cardinalità dell'insieme delle parti di un insieme finito}
  Sia $S_n={a_1,a_2,...,a_n}$ un insieme di $n$ oggetti distinti, allora la cardinalità di $P(S)$ è $ 2^n$

  \chapter{Probabilità sui reali}
  \section{Tribù borelliana}
  Si chiama Tribù Boreliana di $\mathbb{R}$, e si denota con $\mathcal{B}(\mathbb{R})$, la tribù generata su $\mathbb{R}$ dalla classe di tutti gli intervalli $(a,b]$ di $\mathbb{R}$. I suoi elementi si chiamano gli insiemi boreliani di $\mathbb{B}$. e lo spazio $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ è uno spazio probabilizzabile.\\

  \subsubsection{Elementi della tribù Borelliana}
  La tribù di Borel su $\mathbb{R}$ contiene anche i seguenti Elementi
  \begin{itemize}
    \item $(a,b]$
    \item $[a,b]$
    \item $[a,b)$
    \item $(-\infty, b]$
    \item $(a, \infty)$
    \item i singoletti di $\mathbb{R}$
    \item gli insiemi finiti di $\mathbb{R}$
    \item gli insiemi numerabili di $\mathbb{R}$
  \end{itemize}

  \section{Costruzione di una funzione di probabilità su $(\mathbb{R},\mathcal{B}(\mathbb{R}))$}
  Per procedere all'assegnazione di una funzione di Probabilità agli eventi di $\mathcal{B}(\mathbb{R})$, si fissa la probabilità da attribuire agli intervalli $(a,b]$ mediante una funzione $F(x)$ che è
  \begin{itemize}
    \item non decrescente
    \item continua da destra per ogni $x \in \mathbb{R}: \lim_{x\to x_0^+}(x) = F(x_0)$ per ogni $x_0 \in \mathbb{R}$
    \item $\lim_{x\to +\infty} F(x) = 1$
    \item $\lim_{x\to -\infty} F(x) = 0$
  \end{itemize}
  ponendo
  \[ Pr((a,b]) = F(b) - F(a) \]

  Ad ogni insieme di $\mathcal{B}(\mathbb{R})$ è quindi possibile attribuire una probabilità. Il calcolo effettivo di Pr(A) può essere fatto in modo semplice quando $A$ è
  \begin{itemize}
    \item un intervallo
    \item un'unione numerabile di intervalli disgiunti
  \end{itemize}
  \[ Pr(\bigcup\limits_{i=1}^{\infty} (a_i,b_i]) = \sum_{i=1}^{\infty} Pr((a_i,b_i]) = \sum_{i=1}^{\infty} (F(b_i)-F(a_i)) \]

  \section{Probabilità condizionale}
  Sia $(\Omega, \mathcal{A}, Pr)$ uno spazio probabilizzato. Fissato un elemento $h$ di $\mathcal{A}$ con $Pr(H) \neq 0$, si chiama funzione di probabilità dedotta da $Pr$ sotto la condizione $H$ la funzione di probabilità $Pr_H$ sullo spazio $(\Omega, \mathcal{A})$ Probabilizzabile
  \[ Pr_H(A) = \frac{Pr(A \cap H)}{Pr(H)} \]
  Per ogni evento $A \in \mathcal{A}$.

  La probabilità $Pr_H(A)$ si chiama \textbf{Probabilità Condizionale} di $A$, secondo $Pr$, sotto la condizione $H$ e si denota
  \[ Pr(A|H) \]

  \section{Classe Completa di eventi}
  Dato uno spazio probabilizzabile $(\Omega, \mathcal{A})$ la famiglia di eventi ${\{A_i\}_\infty^{i=1}}$ è detta Classe Completa se
  \begin{itemize}
    \item $\bigcup\limits_{n=1}^{\infty} A_n = \Omega$
    \item $A_i \cap A_j = \emptyset,  i \neq j$
  \end{itemize}

  \section{Teorema delle Probabilità Totali}
  Sia ${\{A_i\}_\infty^{i=1}}$ una famiglia di eventi che costituisce una Classe Completa di $\Omega$ tale che
  \[ Pr(A_i) > 0, i = 1,2,... \]
  Sia $B$ un qualunque evento. allora
  \[ Pr(B) = \sum_{i=1}^\infty Pr(A_i \cap B) = \sum_{i=1}^\infty Pr(A_i)Pr(B|A_i) \]

  \section{Teorema di Bayes}
  Sia ${\{A_\}}_{i=1}^\infty$ una Classe Completa di eventi tale che:\\
  $$Pr(A_i) > 0, i = 1,2,...$$
  e $B$ un qualunque evento con $Pr(B)>0$. allora
  $$Pr(A_i|B)=\frac{Pr(A_i)Pr(B|A_i)}{\sum_{j=1}^\infty Pr(A_j)Pr(B|A_j)} \qquad j=1,2,...  $$

  \section{Indipendenza stocastica}
  In uno spazio probabilizzato $(\Omega, \mathcal{A}, P)$ due eventi $A,B$ si dicono tra loro stocasticamente indipendenti se e solo se
  $$Pr(A \cap B) = Pr(A) \cdot Pr(B)$$

  In particolare si noti che dati due eventi stocasticamente indipendenti $A, B$ allora:
  $$Pr(A|B) = \frac{Pr(A \cap B)}{Pr(B)} = Pr(A)$$
  e lo stesso vale per $Pr(B|A) = Pr(B)$\\

  La nozione di indipendenza può essere estesa a più di due eventi. Vedi NOTE-B P.61

  \section{Tribù indipendenti}
  Dato uno spazio probabilizzato $(\Omega, \mathcal{A}, Pr)$. Due Tribù contenute in $\mathcal{A}$ si dicono tra loro indipendenti se ogni elemento dell'uno è indipendente da ogni elemento dell'altra.

  \chapter{Variabili Aleatorie}
  Sia dato lo spazio probabilizzabile $(\Omega, \mathcal{A})$. Si dice \textbf{Variabile aleatoria} (v.a.) ogni funzione a valori reali definita in $\Omega, y = X(\omega)$, tale che
  $$ \{\omega \in \Omega : X(\omega) \leq x\} \in \mathcal{A} $$ per ogni valore reale $x$.
  \begin{itemize}
    \item Giova osservare che nella definizione la probabilità non gioca alcun ruolo e che quando $\mathcal{A}$ è la classe di tutti i sottoinsiemi di $\Omega$ la condizione nella definizione è sempre soddisfatta.
    \item Per rendersi conto della necessitò di imporre alla funzione $X(\omega)$ la condizione riportata sopra, basterà dire che, intendendo assegnare una probabilità agli insiemi $\{\omega \in \Omega : X(\omega) \leq x\}$ per ogni reale $x$ ed avendo probabilizzato la classe $\mathcal{A}$, occore che tali insiemi appartengano ad $A$.
  \end{itemize}

  \section{Variabili aleatorie e Tribù}
  Siano $\tilde{\Omega}$ e $\Omega$ due insiemi arbitrari e sia $X: \tilde{\Omega} \rightarrow \Omega$ una funzione. Se $\mathcal{A}$ è una Tribù su $\Omega$ allora:
  $$ \tilde{\mathcal{A}} = \{ X^{-1}(A):A \in \mathcal{A} \} $$
  è una Tribù su $\tilde{\Omega}$.

  \subsection{Teorema 10}
  Siano $\tilde{\Omega}$ e $\Omega$ due insiemi arbitrari e sia $X: \tilde{\Omega} \rightarrow \Omega$ una funzione. Se $\mathcal{A}$ è una Tribù su $\Omega$ allora:
  $$ \tilde{\mathcal{A}} = \{ A \in \subseteq \Omega: X^{-1}(A) \in \tilde{\mathcal{A}}\} $$

  \subsection{Teorema 12}
  Ogni funzione contiuna oppure monotona crescente o decrescente $f:(\mathbb{R},\mathcal{B}(\mathbb{R})) \rightarrow (\mathbb{R},\mathcal{B}(\mathbb{R})) $ è una variabile aleatoria.

  \section{Variabili aleatorie e funzioni di probabilità p.71}
  \section{Variabili aleatorie discrete}
  Una v.a. $X$ definita su $(\Omega, \mathcal{A})$ è detta discreta se i valori distinti dell'insieme $\bigcup_{\omega \in \Omega} \{ {X(\omega)} \}$ costituiscono un insieme $R_X$ finito o numerabile.

  \subsection{Funzione di probabilità (o densità discreta)}
  Se $X$ è una v.a. discreta con $R_X = {x_1,x_2,...}$, allora la funzione, definita in $\mathbb{R}$, data da
  \[
    p(x) =
      \begin{cases}
         Pr(X = x_i) \g 0 & x = x_i \in R_X \\
         0  & x \not\in R_X
      \end{cases}
  \]
  è detta funzione di probabilità (o densità discreta) della v.a.$X$, $R_X$ viene desso supporto della v.a. $X$.

  \subsection{Teorema}
  Se $X$ è una v.a. discreta con $R_X = \{ x_1,x_2,... \}$ allora
  \[ p(x) \geq 0 \] per ogni x reale e \[ \sum_{x\in R_X} p(x) = 1 \]

  \subsection{Distribuzione Binomiale}
  Si dice che una v.a. $X$ si distribuisce secondo la distribuzione di probabilità (o legge) binominale di parametri $N \geq 1$ (intero) e $0 \leq p \leq 1$, se
  \[
    Pr(X = x) =
      \begin{cases}
          \binom{N}{x}p^x(1-p)^{N-x} & x = 0, 1, ..., N \\
          0 & altrimenti
      \end{cases}
  \]
   E scriveremo $ X \sim Bi(N,p) $, dove $n$ è il numero di prove effettuate, e $p$ è la probabilità di successo della singola prova.
   \subsubsection{In altre parole}
   La distibuzione binomiale descrive la probabilità di avere esattamente $x$ successi, provando $N$ volte, con $p$ probabilità di vittoria di un singolo evento.

   \subsubsection{Propietà}
   \begin{itemize}
     \item Media: $\mathbb{E}(X) = Np$
     \item Varianza: $\mathbb{V}ar(X) = Np(1-p)$
   \end{itemize}

   \subsection{Funzione di ripartizione}
   Sia $X$ una v.a.. Si dice funzione di ripartizione della v.a. $X$ la funzione $y=F(x)$, definita per ogni $x$ reale, data da
   \[ F(x) = Pr(X \leq x) \quad x \in \mathbb{R} \]

  \subsubsection{Funzione di ripartizione e funzione di probabilità}
  Per una v.a. discreta, si osservi, a conferma delle propietà generali della funzione di ripartizione, come i punti di discontinuità di $F(x)$ coincidano con i punti di $R_X$ della v.a. e che l'ampiezza del salto in detti punti corrisponde alla funzione di probabilità, cioè
  \[ p(X=x) = F(x) - F(X^-) \]

  \subsection{Distribuzione Geometrica}
  La distribuzione Geometrica nasce con riferimento allo stesso schema che ha condotto alla distribuzione Binomiale ma ora, anzichè contare il numero di successi in $N$ prove indipendenti, interessa il numero delle prove necessarie per ottenere il primo successo.\\

  Si dice che una v.a. $X$ si distribuisce secondo una distribuzione geometrica di parametro $0 \leq p \leq 1$ se la sua funzione di probabilità è

  \[
    Pr(X=x)=
      \begin{cases}
        p(1-p)^{x-1} & x = 1,2,3,... \\
        0 & altrove
      \end{cases}
  \]
  e scriveremo $X \sim Ge(p)$.
  \subsubsection{Propietà}
  \begin{itemize}
    \item Funzione di ripartizione: $F(x) =   1-(1-p)^x$
    \item Momento secondo: $ \mathbb{E}(X^2) = \frac{2-p}{p^2} $
    \item Varianza: $ \mathbb{V}ar(X) = \mathbb{E}(X^2) - [\mathbb{E}(X)]^2 = \frac{1-p}{p^2} $
  \end{itemize}



  \subsection{Distribuzione Binomiale negativa (o di Pascal)}
  Si dice che una v.a. $X$ si distribuisce secondo la distribuzione binomiale negativa di parametri $0 < p \leq 1$ e $r \geq 1$ (intero) se la sua funzione di probabilità è data da

  \[
    Pr(X=x)=
      \begin{cases}
          \binom{x-1}{r-1} p^r (1-p)^{x-r} & x=r, r+1, r+2,... \\
          0 & altrove
      \end{cases}
  \]

  e indichiamo con $X \sim BiNe(r,p)$.

  \subsubsection{In altre parole}
  La distribuzione di Pascal dà la probabilità che siano necessari esattamente $x$ fallimenti per avere $r$ successi. $p$ è la probabilità di un singolo successo.

  \subsubsection{Relazione tra Binomiale e Binomiale negativa (Teorema)}
  Sia $X \sim BiNe(r,p)$ e $Z \sim Bi(N,p)$ allora
  \[ Pr(Z \geq r) = Pr(X \leq N) \]

  \section{Variabili aleatorie continue}
  Una v.a. $X$ definita su $(\Omega, \mathcal{A})$ è detta continua se la sua funzione di ripartizione è continua.

  \subsection{Densità}
  Si dice che la v.a. $X$ è dotata di densità se la probabilità con cui $X$ assume valori nell'intervallo $(a,b]$ è data mediante la formula

  \[ Pr(X \in (a,b]) = Pr(a < X \leq b) = \int_{a}^{b} f(x)  dx \]
  in cui $f(x)$ prende il nome di funzione di densità di probabilità della v.a. $X$ e deve avere le seguenti caratteristiche
  \begin{itemize}
    \item $f(x) > 0$ per ogni $x \in \mathbb{R}$
    \item $\int_{-\infty}^{+\infty} f(x) dx = 1$
  \end{itemize}

  \subsection{Variabili aleatorie assolutamente continue}
  Una v.a. $X$ definita su $(\Omega, \mathcal{A})$ è detta assolutamente continua se la sua funzione di ripartizione è continua e la sua v.a. $X$ ammette densità.

  \subsection{Densità e funzione di ripartizione}
  Per una v.a. $X$ assolutamente continua con densità $f(x)$ e con funzione di ripartizione $F(x)$ abbiamo:
  \[ Pr(X \in (a,b]) = \int_{a}^{b} f(x) dx = F(b) - F(a) \]

  \subsection{Distribuzione Normale (o di Gauss)}
  Si dice che una v.a. $X$ si distribuisce con legge di probabilità Normale (o Gaussiana) di parametri $-\infty < \mu < +\infty$ e $ 0 < \sigma < +\infty$ se possiede la seguente densità.

  \[ f(x,\mu,\sigma) = \frac{1}{\sqrt{(2 \pi \sigma^2)}} e^{( -\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2})} \]

  e la indichiamo con $X \sim N(\mu, \sigma^2)$. La v.a. $X \sim N(0,1)$ è chiamata Normale Standard.

  \subsubsection{Proprietà}
  \begin{itemize}
    \item Valore atteso: $\mathbb{E}(X) = \mu$
    \item Varianza: $\mathbb{V}ar(X) = \sigma^2$
  \end{itemize}

  \subsection{Standardizzazione di una Normale}
  Data una $X \sim N(\mu,\sigma^2)$, Allora
  \[ Z = \frac{X-\mu}{\sigma} \sim N(0,1) \]
  Questa operazione viene chiamata Standardizzazione


  \subsection{Distribuzione Esponenziale}
  Si dice che una v.a. $X$ ha legge Esponenziale con parametro $\lambda >0$ se la sua funzione di densità
  \[
    f(x;\lambda) =
    \begin{cases}
      \lambda e^{(-\lambda x)} & x > 0 \\
      0 & altrove
    \end{cases}
  \]

  e la indichiamo nel seguente modo $X \sim Exp(\lambda)$. La distribuzione Esponenziale è senza memoria.
  \subsubsection{Propietà}
  \begin{itemize}
    \item Media: $\mathbb{E}(X) = \frac{1}{\lambda}$
    \item Varianza: $\mathbb{V}ar(X) = \frac{1}{\lambda^2}$
    \item Funzione di ripartizione: $F(x) = 1-e^{-\lambda x}$
    \item Il minimo $Y = min\{X_1,...,X_n\} $ tra $n$ variabili aleatorie indipendenti con distribuzioni esponenziali di parametri $ \lambda_1,...,\lambda_n $ è ancora una variabile aleatoria con distribuzione esponenziale, di parametro $ \lambda = \lambda_1 + ... + \lambda_n$.
  \end{itemize}


  \subsection{Trasformazione di variabili aleatorie p.104 (manca)}

  \section{Speranza matematica o valore atteso per v.a. discrete}
  Sia $X$ una v.a. discreta con funzione di probabilità $p_X(x)$. Allora, si chiama speranza matematica di $X$ la quantità (finita)
  \[ \mathbb{E}(X) = \sum_{x \in R_X} x p_X(x) \]\\

  Sia $X$ una v.a. dotata di densità $f_X(x)$ e funzione di ripartizione $F_X(x)$. Si chiama speranza matematica di $X$ la quantità (finita).
  \[ \mathbb{E}(X) = \int_{-\infty}^{+\infty} x f_X(x) dx \]

  \section{Momenti}
  Data la v.a. $X$ si dice momento non centrato di ordine $r$ (intero positivo) il valore
  \[ \mu_r = \mathbb{E}(X^r)\]
  e si dice momento centrato dalla media di ordine r
  \[ \bar{\mu_r} = \mathbb{E}((x-\mu_1)^r)  \]

  \subsubsection{Valori di sintesi basati sui momenti}
  \begin{itemize}
    \item Media: $ \mu = \mu_1 = \mathbb{E}(X) $
    \item Varianza: $ \mathbb{V}ar(X) = \sigma^2 = \bar{\mu_2} = \mathbb{E}((x-\mu_1)^2) = \mathbb{E}(X^2) - \mathbb{E}(X)^2 $
    \item Deviazione standard: $ \sigma = \sqrt{\sigma^2} $
  \end{itemize}

  \chapter{Variabili Aleatorie Doppie}
  Sia $(\Omega, \mathcal{A}, Pr)$ uno spazio probabilizzato. Siano $X(\omega)$ e $Y(\omega)$ due v.a. definite su $\Omega$ in modo che:
  \[ Z(\omega) = (X(\omega),Y(\omega)) : \Omega \rightarrow \mathbb{R}^2 \]

    $Z(\omega)$ è detta v.a. doppia e $R_Z = R_{X,Y} = \{(x,y):x \in R_X,y \in R_Y\}$\\

  Resta da definire la funzione di probabilità di $Z(\omega)$. Le funzioni di ripartizione $F_X(x)$ e $F_Y(y)$ di $X$ e $Y$ rispettivamente, in genere non sono sufficienti per determinare tale propietà.

    E' necessario considerare la seguente funzione di ripartizione (detta congiunta)
  \[ F_Z(z) = F_{X,Y}(x,y) = Pr(\{ X \leq x\} \cap \{ Y \leq y \}) \quad\quad (x,y)\in R_{X,Y} \]

  \section{Funzione di probabilità congiunta (discreta)}
  Per due v.a. discrete $X$ e $Y$, la v.a. doppia $Z=(X,Y)$ (che è discreta) ha funzione di probabilità (congiunta)

  \[
    P_Z(z) =
    \begin{cases}
      p_{X,Y}(x,y) = Pr(\omega : \{ X(\omega) = x \} \cap \{ Y(\omega) = y \}) & (x,y) \in R_{X,Y}\\
      0 & altrove
    \end{cases}
  \]

  \section{Variabili aleatorie doppie dotate di densità}
  La v.a. doppia $Z= (X,Y)$ si dirà dotata di densità se esiste una funzione $f_{X,Y}(x,y)$ tale che
  \begin{itemize}
    \item $f_{X_Y}(x,y) \geq 0, \quad \forall (x,y) \in \mathbb{R}^2$
    \[ \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} f_{X,Y}(x,y)\: dx\: dy \]
    \item \[Pr(a < x \leq b, c < y \leq d) = \int_{-\infty}^{x}\int_{-\infty}^{y} f_{X,Y}(u,v)\: du\: dv \]

  \end{itemize}
  tale funzione è chiamata densità congiunta $f_Z(z) = f_{X,Y}(x,y)$.

  \subsection{Densità marginali}
  Dalle formule di prima abbiamo che
  \[ F_{X,Y}(x,y) = \int_{-\infty}^{x}\int_{-\infty}^{y} f_{X,Y}(u,v)\: du\: dv\]
    E quindi
  \begin{itemize}
    \item Densità marginale della v.a. $X$ \[f_X(x) = \int_{-\infty}^{+\infty} f_{X,Y}(x,v)\: dv\]
    \item Denstià marginale della v.a. $Y$ \[f_Y(y) = \int_{-\infty}^{+\infty} f_{X,Y}(u,y)\: du\]
  \end{itemize}

  \section{Distribuzioni condizionali per v.a. (Probabilità condizionale di $X|Y = y$)}
  Sia $(X,Y)$ una v.a. doppia discreta con funzione di probabilità
  \[ p_{X,Y}(x,y) = Pr(X = x, Y = y) \]
    allora in accordo con la definizione di probabilità condizionale

  \[ p_{X_Y}(X=x|Y=y) = Pr(\{ X=x \}|\{ Y=y \}) = \frac{p_{X,Y}(x,y)}{p_Y(y)} \quad y \in R_Y(p_Y(y)>0) \]
  Per ogni valore fissato di $y \in R_Y$ la funzione $p_{X|Y}(X=x|Y=y)$ prende il nome di probabilità condizionale di $X|Y = y$

  \section{Distribuzioni condizionali e indipendenza per v.a. (p137 dispense B)}

  \section{Funzioni di ripartizioni condizionali}
  Dalla funzione di probabilità confizionale (nel caso discreto) e dalla densità condizionale (nel caso assolutamente continuo), possiamo costruire le funzioni di ripartizione condizionale
  \[ F_{X|Y}(x|y) = \sum_{u \leq x \: : \: \in R_X} p_{X|Y}(u|y) \]
  e
  \[ F_{X|Y}(x|y) = \int_{-\infty}^{x} f_{X|Y}(u|y) \: du \]

  \section{Variabili aleatorie condiionali e speranza matematica}
  Data la v.a. doppia $(X,Y)$ allora la funzione $X|Y = y \: (y \in R_Y)$ è una v.a. con funzione di probabilità $P_{X|Y}(x|y)$.
    Quindi alla definizione di speranza matematica e di varianza abbiamo

  \[\mathbb{E}(X|Y=y)= \quad \sum_{x \in R_X} x p_{X|Y}(x|y) \]
  \[ var(X|Y=y)= \quad \sum_{x\in R_X}(x - \mathbb{E}(X|Y=y))^2 p_{X|Y}(x|y) \]
  e in maniera del tutto analoga nel caso di v.a. dotate di densità.

  \section{Speranza matematica della speranza matematica condizionale}
  Ad esempio per v.a. doppie discrete (??il risultato vale nel caso generale??)
  \[ \mathbb{E}(\mathbb{E}(X|Y)) = \mathbb{E}(X) \]

  \section{Varianza e Varianza condizionale (Scomposizione della varianza)}
  Sia $(X,Y)$ una v.a doppia, allora
  \[ \mathbb(V)ar(X) = \mathbb{E}(\mathbb{V}ar(X|Y)) + \mathbb{V}ar(\mathbb{E}(X|Y)) \]

  \section{Dipendenza in media}
  La v.a. $X$ si dice indipendente in media da $Y$ se
  \[ \mathbb{E}(X|Y=y) = \mathbb{E}(X) \quad \forall y \in R_Y \]

  Si noti che se $X$ è indipendente stocasticamente da Y allora è anche indipendente in media. Viceversa non è vero, in generale.

  \section{Rapporto di correlazione}
  Sia $(X,Y)$ una v.a. doppia discreta, si chiama rapporto di correlazione di $X$ dato $Y$
  \[ \eta^2_{X|Y} = \frac{\mathbb{V}ar(\mathbb{E}(X|Y))}{\mathbb{V}ar(X)} = 1- \frac{\mahtbb{E}(\mathbb{V}ar(X|Y))}{\mathbb{V}ar(X)} \quad \mathbb{V}ar(X) > 0\]

  E in modo analogo si definisce $\eta_{X|Y}^2$. Dalla formula della scomposizione della varianza è facile vedere che:
  \[ 0 \leq \eta_{X|Y}^2 \leq 1 \]

  inoltre
  %Elenco puntato invece che numerato
  \begin{itemize}
    \item se $\eta_{X|Y}^2 = 0$ allora $X$ è indipendente in media da $Y$
    \item se $\eta_{X|Y}^2 > 0$ allora $X$ è indipendente in media da $Y$
    \item $\eta_{X|Y}^2 = 1$ se e solo se $Pr(X = \mathbb{E}(X|Y)) = 1$
  \end{itemize}

  %modificare il simbolo di prodotto con uno più decente
  \section{Covarianza e correlazione}
  La covarianza e la correlazione sono altri due indici di dipendenza (lineare) tra due v.a.
  \[ cov(X,Y) = \mathbb{E}(X * Y) - \mathbb{E}(X) * \mathbb{E}(Y) \]
  mentre
  \[ \rho(X,Y) = \frac{cov(X,Y)}{\sqrt{\mathb{V}ar(X)*\mathbb{V}ar(Y)}} \]

  \section{Varianza di una combinazione lineare di v.a}
  Sia $(X,Y)$ una v.a. doppia e $a$ e $b$ due costanti. Allora
  \[ \mathbb{V}ar(aX + bY)= a^2\mathbb{V}ar(X) + b^2\mathbb{V}ar(Y) + 2abcov(X,Y) \]

  %Skippato il chi^2 di pearson p.143 disp. B

  \chapter{Teoremi limite della probabilità}
  \section{Convergenza in probabilità (o debole)}
  Ci sono diversi modi per esprimere il fatto che $S_n/n$ si avvicina a $p$. Potremmo ad esempio scrivere che, per $n$ grande e per $\epsilon$ piccolo a piacere

  \[ Pr\{ |S_n/n -p| \geq \epsilon \} \approx 0 \]
  o equivalentemente
  \[ \lim_{n \rightarrow +\infty} Pr\{ |S_n/n -p| \geq \epsilon \} = 0 \]
  in simboli questo tipo di convergenza si denota con
  %Trovare come scrivere la p sopra la freccia
  \[ p \]
  \[ S_n/n \longrightarrow \mu \]

  e si legge \textbf{converge in probabilità (o in senso debole)} ad una v.c. $Y$ se, per ogni $\epsilon > 0$,

  \[ \lim_{n \to \infty} Pr(|Y_n - Y| \geq \epsilon) = 0, \]
  ovvero
  \[ \lim_{n \to \infty} Pr(|Y_n - Y| \leq \epsilon) = 1, \]

  \section{Convergenza in media quadratica}
  Un'altra formalizzazione del concetto di "vicinanza" potrebbe richiedere che in media gli scostamenti (al quadrato) di $S_n/n$ da $p$ siano piccoli, quando $n$ è grande:
  \[\mathbb{E}[(S_n/n - p)^2] \approx 0, \]
  o equivalentemente
  \[ \lim_{n \to \infty} \mathbb{E}[(S_n/n - p)^2] = 0. \]
  In simboli questo tipo di convergenza si denota con
  %mettere m.q. sopra la freccia
  \[ m.q. \]
  \[ S_n/n \longrightarrow p \]
  e si legge "\textbf{converge in media quadratica}".\\
  Più in generale diremo che una successione $Y_1,Y_2,...$ \textbf{converge in media quadratica} ad una v.c. $Y$ se
  \[ \lim_{n \to \infty} \mathbb{E}[(Y_n - Y)^2] = 0. \]

  \subsubsection{Proposizione}
  La convergenza in media quadratica implica la convergenza in Probabilità:
  \[ m.q. \quad \quad \quad P\]
  \[ Y_n \rightarrow Y \Rightarrow Y_n \rightarrow Y \]

  \section{Disuguaglianza di Markov}
  Sia $Y$ una v.c. che assume valori non negativi allora per ogni numero reale $a>0$
  \[ Pr(Y \geq a) \leq \frac{\mathbb{E}(Y)}{a} \]

  \section{Disuguaglianza di Chebychev}
  Sia $Y$ una v.c. con valore atteso $\mathbb{E}(Y) = \mu$ e varianza $\mathbb{V}ar(Y) = \sigma^2$. Allora
  \[ Pr(|Y-\mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2} \]

  \section{Somme di variabili casuali}
  \subsubsection{Proposizione}
  Siano $Y_1,...,Y_n$ v.c. con valore atteso rispettivamente $\mu_1,...,\mu_n$. allora
  \[ \mathbb{E}(Y_1 + ... + Y_n) = \mu_1 + ... + \mu_n \]

  \subsubsection{Proposizione}
  Siano $Y_1,...,Y_n$ v.c. indipendenti con varianza $\sigma_1^2, ..., \sigma_n^2$ rispettivamente. Allora
  \[ \mathbb{V}ar(Y_1 + ... + Y_n) = \sigma_1^2 + ... + \sigma_n^2 \]

  \subsubsection{Proposizione}
  Siano $Y_1,...,Y_n$ v.c. indipendenti, tutte con valore atteso $\mu$ e varianza $\sigma^2$ e sia $\overline{Y}_n = \sum_{i = 1}^n Y_i/n.$ Allora
  \[ \mathbb{E}(\overline{Y}_n) = \sum_{i = 1}^n \frac{\mathbb{E}(Y_i)}{n} = n\frac{\mu}{n}= \mu,\]
  \[ \mathbb{V}ar(\overline{Y}_n)= \sum_{i=1}^n \frac{\mathbb{V}ar(Y_i)}{n^2} = n\frac{\sigma^2}{n^2} = \frac{\sigma^2}{n}. \]

  \section{Legge debole dei grandi numeri}
  Sia $Y_1, Y_2, ...$ una successione di v.c. indipendenti, ciascuna con valore atteso $\mu$ e varianza $\sigma^2$. Allora, per ogni $\epsilon > 0$,
  \[ lim_{n \to \infty} Pr\{ |\overline{Y}_n -\mu| \geq \epsilon \} = 0 \]
  %aggiungere P sopra la freccia
  ovvero $\overline{Y}_n \rightarrow \mu$

  p.151




















\end{document}
